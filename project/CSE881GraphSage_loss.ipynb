{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import SAGEConv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic GraphSAGE model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_units):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_features, hidden_units)\n",
    "        self.conv2 = SAGEConv(hidden_units, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(): # the same as used with GCN\n",
    "    adj = sp.load_npz('adj.npz')\n",
    "    features = np.load('features.npy')\n",
    "    labels = np.load('labels.npy')\n",
    "    with open('splits.json', 'r') as file:\n",
    "        splits = json.load(file)\n",
    "    idx_train, idx_test = splits['idx_train'], splits['idx_test']\n",
    "    # convert adjacency matrix to edge index\n",
    "    adj = adj.tocoo()\n",
    "    edge_index = np.vstack((adj.row, adj.col))\n",
    "\n",
    "    # normalize features\n",
    "    features = features / features.sum(1, keepdims=True)\n",
    "\n",
    "    # convert to tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "    features = torch.tensor(features, dtype=torch.float)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # create torch geometric data object\n",
    "    data = Data(x=features, edge_index=edge_index, y=labels)\n",
    "\n",
    "    return data, torch.tensor(idx_train, dtype=torch.long), torch.tensor(idx_test, dtype=torch.long)\n",
    "\n",
    "data, idx_train, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to optimize on loss, rather than accuracy, due to the tiny size of the labelled training data we have available. Hopefully we will be better about avoiding over-fitting this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 0: Training Loss 1.9426805973052979, Validation Loss 1.9149760007858276, Validation Accuracy: 0.14\n",
      "Fold 1, Epoch 10: Training Loss 1.7043107390403747, Validation Loss 1.5302152633666992, Validation Accuracy: 0.34\n",
      "Fold 1, Epoch 20: Training Loss 1.0492207944393157, Validation Loss 0.8729313015937805, Validation Accuracy: 0.78\n",
      "Fold 1, Epoch 30: Training Loss 0.43951942324638366, Validation Loss 0.4909323453903198, Validation Accuracy: 0.88\n",
      "Fold 1, Epoch 40: Training Loss 0.1713249646127224, Validation Loss 0.42892152070999146, Validation Accuracy: 0.86\n",
      "Fold 1, Epoch 50: Training Loss 0.07042262591421604, Validation Loss 0.4346557557582855, Validation Accuracy: 0.84\n",
      "Fold 1, Epoch 60: Training Loss 0.033582423254847525, Validation Loss 0.4620679020881653, Validation Accuracy: 0.82\n",
      "Fold 1, Epoch 70: Training Loss 0.020745658315718174, Validation Loss 0.5022260546684265, Validation Accuracy: 0.82\n",
      "Fold 1, Epoch 80: Training Loss 0.015056735463440418, Validation Loss 0.5055391788482666, Validation Accuracy: 0.82\n",
      "Fold 1, Epoch 90: Training Loss 0.011635609436780214, Validation Loss 0.5275281667709351, Validation Accuracy: 0.82\n",
      "Fold 1, Epoch 100: Training Loss 0.009930651867762208, Validation Loss 0.5436225533485413, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 110: Training Loss 0.00799493002705276, Validation Loss 0.5517274141311646, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 120: Training Loss 0.006217268016189337, Validation Loss 0.549675464630127, Validation Accuracy: 0.82\n",
      "Fold 1, Epoch 130: Training Loss 0.0055629196111112835, Validation Loss 0.5593820810317993, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 140: Training Loss 0.005869388300925493, Validation Loss 0.5636565089225769, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 150: Training Loss 0.00495738519821316, Validation Loss 0.5659993290901184, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 160: Training Loss 0.00385794413741678, Validation Loss 0.5774640440940857, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 170: Training Loss 0.004116410273127258, Validation Loss 0.5867283344268799, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 180: Training Loss 0.004139645560644567, Validation Loss 0.5896928310394287, Validation Accuracy: 0.8\n",
      "Fold 1, Epoch 190: Training Loss 0.0034136898117139935, Validation Loss 0.5952106714248657, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 0: Training Loss 1.9472090005874634, Validation Loss 1.901486873626709, Validation Accuracy: 0.28\n",
      "Fold 2, Epoch 10: Training Loss 1.6526998043060304, Validation Loss 1.4839917421340942, Validation Accuracy: 0.4\n",
      "Fold 2, Epoch 20: Training Loss 0.9206378877162933, Validation Loss 0.8661681413650513, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 30: Training Loss 0.3584806650876999, Validation Loss 0.6068039536476135, Validation Accuracy: 0.82\n",
      "Fold 2, Epoch 40: Training Loss 0.13544081524014473, Validation Loss 0.6039158701896667, Validation Accuracy: 0.82\n",
      "Fold 2, Epoch 50: Training Loss 0.054980213195085524, Validation Loss 0.6540022492408752, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 60: Training Loss 0.028206098452210428, Validation Loss 0.6772494316101074, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 70: Training Loss 0.01774668078869581, Validation Loss 0.6968340873718262, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 80: Training Loss 0.013170277886092662, Validation Loss 0.7248932719230652, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 90: Training Loss 0.011216549202799797, Validation Loss 0.7365145683288574, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 100: Training Loss 0.008255772711709141, Validation Loss 0.7486774921417236, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 110: Training Loss 0.007521773874759674, Validation Loss 0.7625820636749268, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 120: Training Loss 0.006284541729837656, Validation Loss 0.7740505933761597, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 130: Training Loss 0.005654144892469048, Validation Loss 0.7801236510276794, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 140: Training Loss 0.0050670219585299495, Validation Loss 0.7954807877540588, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 150: Training Loss 0.004802017868496477, Validation Loss 0.8106752634048462, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 160: Training Loss 0.004381103441119194, Validation Loss 0.8169847726821899, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 170: Training Loss 0.004013481945730746, Validation Loss 0.8237563371658325, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 180: Training Loss 0.0034081883262842894, Validation Loss 0.835888147354126, Validation Accuracy: 0.8\n",
      "Fold 2, Epoch 190: Training Loss 0.0034525941824540495, Validation Loss 0.8413691520690918, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 0: Training Loss 1.9539307355880737, Validation Loss 1.9348804950714111, Validation Accuracy: 0.28\n",
      "Fold 3, Epoch 10: Training Loss 1.6842992305755615, Validation Loss 1.3925528526306152, Validation Accuracy: 0.42\n",
      "Fold 3, Epoch 20: Training Loss 0.988411808013916, Validation Loss 0.7580929398536682, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 30: Training Loss 0.40022020041942596, Validation Loss 0.5221407413482666, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 40: Training Loss 0.14980222284793854, Validation Loss 0.484975129365921, Validation Accuracy: 0.84\n",
      "Fold 3, Epoch 50: Training Loss 0.06409559696912766, Validation Loss 0.5008077025413513, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 60: Training Loss 0.03053999673575163, Validation Loss 0.515998899936676, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 70: Training Loss 0.0178399414755404, Validation Loss 0.5335912704467773, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 80: Training Loss 0.01387952733784914, Validation Loss 0.5506900548934937, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 90: Training Loss 0.01035126056522131, Validation Loss 0.5638712048530579, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 100: Training Loss 0.008580667991191148, Validation Loss 0.5922094583511353, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 110: Training Loss 0.006900652172043919, Validation Loss 0.597645103931427, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 120: Training Loss 0.0060833081603050235, Validation Loss 0.6066245436668396, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 130: Training Loss 0.005467667151242495, Validation Loss 0.6160613298416138, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 140: Training Loss 0.004792388202622533, Validation Loss 0.6425310373306274, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 150: Training Loss 0.004129321197979152, Validation Loss 0.6408693790435791, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 160: Training Loss 0.004303787252865732, Validation Loss 0.6406902074813843, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 170: Training Loss 0.0032858306309208273, Validation Loss 0.6347680687904358, Validation Accuracy: 0.8\n",
      "Fold 3, Epoch 180: Training Loss 0.0029591282829642294, Validation Loss 0.6385555863380432, Validation Accuracy: 0.82\n",
      "Fold 3, Epoch 190: Training Loss 0.0029258030466735364, Validation Loss 0.6571758389472961, Validation Accuracy: 0.8\n",
      "Fold 4, Epoch 0: Training Loss 1.9375958442687988, Validation Loss 1.90805983543396, Validation Accuracy: 0.32\n",
      "Fold 4, Epoch 10: Training Loss 1.684671437740326, Validation Loss 1.4360055923461914, Validation Accuracy: 0.38\n",
      "Fold 4, Epoch 20: Training Loss 0.9925270557403565, Validation Loss 0.8707743883132935, Validation Accuracy: 0.74\n",
      "Fold 4, Epoch 30: Training Loss 0.3994982182979584, Validation Loss 0.6413543224334717, Validation Accuracy: 0.82\n",
      "Fold 4, Epoch 40: Training Loss 0.15095755457878113, Validation Loss 0.631071925163269, Validation Accuracy: 0.82\n",
      "Fold 4, Epoch 50: Training Loss 0.06204463168978691, Validation Loss 0.6576234698295593, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 60: Training Loss 0.030588017031550407, Validation Loss 0.7161194682121277, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 70: Training Loss 0.0180273893289268, Validation Loss 0.7118983268737793, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 80: Training Loss 0.014432953950017691, Validation Loss 0.7445390224456787, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 90: Training Loss 0.010724310018122196, Validation Loss 0.7516849637031555, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 100: Training Loss 0.009398435149341822, Validation Loss 0.7518370747566223, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 110: Training Loss 0.008064753143116832, Validation Loss 0.7888627648353577, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 120: Training Loss 0.006011540908366442, Validation Loss 0.803135097026825, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 130: Training Loss 0.006092900037765503, Validation Loss 0.7868374586105347, Validation Accuracy: 0.76\n",
      "Fold 4, Epoch 140: Training Loss 0.005761788971722126, Validation Loss 0.7958412766456604, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 150: Training Loss 0.004247481585480273, Validation Loss 0.8247637748718262, Validation Accuracy: 0.76\n",
      "Fold 4, Epoch 160: Training Loss 0.004646968189626932, Validation Loss 0.8177878856658936, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 170: Training Loss 0.004270116868428886, Validation Loss 0.8112679123878479, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 180: Training Loss 0.003228483023121953, Validation Loss 0.8434921503067017, Validation Accuracy: 0.78\n",
      "Fold 4, Epoch 190: Training Loss 0.0032447691541165114, Validation Loss 0.8525022864341736, Validation Accuracy: 0.78\n",
      "Fold 5, Epoch 0: Training Loss 1.949296236038208, Validation Loss 1.9174613952636719, Validation Accuracy: 0.32\n",
      "Fold 5, Epoch 10: Training Loss 1.66709566116333, Validation Loss 1.4403908252716064, Validation Accuracy: 0.46\n",
      "Fold 5, Epoch 20: Training Loss 0.9279855787754059, Validation Loss 0.9030227661132812, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 30: Training Loss 0.3526506170630455, Validation Loss 0.758799135684967, Validation Accuracy: 0.78\n",
      "Fold 5, Epoch 40: Training Loss 0.13682070523500442, Validation Loss 0.8545784950256348, Validation Accuracy: 0.78\n",
      "Fold 5, Epoch 50: Training Loss 0.05790144726634026, Validation Loss 0.9146272540092468, Validation Accuracy: 0.78\n",
      "Fold 5, Epoch 60: Training Loss 0.029585126787424088, Validation Loss 0.9691867232322693, Validation Accuracy: 0.76\n",
      "Fold 5, Epoch 70: Training Loss 0.01934589063748717, Validation Loss 1.0188385248184204, Validation Accuracy: 0.76\n",
      "Fold 5, Epoch 80: Training Loss 0.012865705881267786, Validation Loss 1.031343698501587, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 90: Training Loss 0.010931238904595375, Validation Loss 1.032701015472412, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 100: Training Loss 0.009153205575421453, Validation Loss 1.0669995546340942, Validation Accuracy: 0.76\n",
      "Fold 5, Epoch 110: Training Loss 0.008415439026430249, Validation Loss 1.085395336151123, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 120: Training Loss 0.006681344192475081, Validation Loss 1.1073006391525269, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 130: Training Loss 0.006182796834036708, Validation Loss 1.1157809495925903, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 140: Training Loss 0.005570025276392698, Validation Loss 1.1289559602737427, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 150: Training Loss 0.003993988921865821, Validation Loss 1.142470121383667, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 160: Training Loss 0.004287702171131969, Validation Loss 1.145522952079773, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 170: Training Loss 0.003512073680758476, Validation Loss 1.1544517278671265, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 180: Training Loss 0.0035545673687011003, Validation Loss 1.1683467626571655, Validation Accuracy: 0.74\n",
      "Fold 5, Epoch 190: Training Loss 0.0031242653029039504, Validation Loss 1.1824177503585815, Validation Accuracy: 0.74\n",
      "Fold 6, Epoch 0: Training Loss 1.9484070539474487, Validation Loss 1.9142887592315674, Validation Accuracy: 0.26\n",
      "Fold 6, Epoch 10: Training Loss 1.6833344459533692, Validation Loss 1.5097036361694336, Validation Accuracy: 0.42\n",
      "Fold 6, Epoch 20: Training Loss 0.9481546521186829, Validation Loss 0.7882270216941833, Validation Accuracy: 0.86\n",
      "Fold 6, Epoch 30: Training Loss 0.33581155687570574, Validation Loss 0.48014912009239197, Validation Accuracy: 0.9\n",
      "Fold 6, Epoch 40: Training Loss 0.11700031459331513, Validation Loss 0.4884008467197418, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 50: Training Loss 0.045843558944761754, Validation Loss 0.5335620641708374, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 60: Training Loss 0.021446978859603404, Validation Loss 0.5559149384498596, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 70: Training Loss 0.014789102971553803, Validation Loss 0.5779133439064026, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 80: Training Loss 0.010738956648856402, Validation Loss 0.6079509258270264, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 90: Training Loss 0.008976541366428137, Validation Loss 0.610098123550415, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 100: Training Loss 0.007167848479002714, Validation Loss 0.6190630197525024, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 110: Training Loss 0.006432063598185778, Validation Loss 0.6278951168060303, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 120: Training Loss 0.005416211253032088, Validation Loss 0.6405032873153687, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 130: Training Loss 0.004474355978891254, Validation Loss 0.6584051251411438, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 140: Training Loss 0.004326820001006126, Validation Loss 0.6726450324058533, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 150: Training Loss 0.004503162950277329, Validation Loss 0.6747041940689087, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 160: Training Loss 0.003444897988811135, Validation Loss 0.6816785335540771, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 170: Training Loss 0.003308623027987778, Validation Loss 0.6996686458587646, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 180: Training Loss 0.003073684615083039, Validation Loss 0.7028510570526123, Validation Accuracy: 0.88\n",
      "Fold 6, Epoch 190: Training Loss 0.0028876197757199407, Validation Loss 0.6981289386749268, Validation Accuracy: 0.88\n",
      "Fold 7, Epoch 0: Training Loss 1.9354974031448364, Validation Loss 1.9211091995239258, Validation Accuracy: 0.42857142857142855\n",
      "Fold 7, Epoch 10: Training Loss 1.6916993618011475, Validation Loss 1.5569024085998535, Validation Accuracy: 0.32653061224489793\n",
      "Fold 7, Epoch 20: Training Loss 1.0520517945289611, Validation Loss 0.9766334295272827, Validation Accuracy: 0.6938775510204082\n",
      "Fold 7, Epoch 30: Training Loss 0.4604757249355316, Validation Loss 0.5899074673652649, Validation Accuracy: 0.8571428571428571\n",
      "Fold 7, Epoch 40: Training Loss 0.17761510685086251, Validation Loss 0.5539436936378479, Validation Accuracy: 0.8571428571428571\n",
      "Fold 7, Epoch 50: Training Loss 0.06954504176974297, Validation Loss 0.5325446128845215, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 60: Training Loss 0.03620530422776937, Validation Loss 0.5669098496437073, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 70: Training Loss 0.020123352017253637, Validation Loss 0.578922688961029, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 80: Training Loss 0.015127989742904902, Validation Loss 0.5762252807617188, Validation Accuracy: 0.8571428571428571\n",
      "Fold 7, Epoch 90: Training Loss 0.011594904121011496, Validation Loss 0.5769940614700317, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 100: Training Loss 0.00978928580880165, Validation Loss 0.5914544463157654, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 110: Training Loss 0.007731639547273517, Validation Loss 0.6093956232070923, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 120: Training Loss 0.007478603860363364, Validation Loss 0.6118350625038147, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 130: Training Loss 0.005955836642533541, Validation Loss 0.6148282289505005, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 140: Training Loss 0.0059044055175036195, Validation Loss 0.6368837356567383, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 150: Training Loss 0.005519520328380168, Validation Loss 0.6418702006340027, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 160: Training Loss 0.0043935254914686086, Validation Loss 0.6427175998687744, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 170: Training Loss 0.0042931490112096075, Validation Loss 0.647566020488739, Validation Accuracy: 0.8163265306122449\n",
      "Fold 7, Epoch 180: Training Loss 0.003711509797722101, Validation Loss 0.65992271900177, Validation Accuracy: 0.8367346938775511\n",
      "Fold 7, Epoch 190: Training Loss 0.003646699246019125, Validation Loss 0.6472463607788086, Validation Accuracy: 0.8367346938775511\n",
      "Fold 8, Epoch 0: Training Loss 1.9563945531845093, Validation Loss 1.9396371841430664, Validation Accuracy: 0.08163265306122448\n",
      "Fold 8, Epoch 10: Training Loss 1.6970072627067565, Validation Loss 1.572365164756775, Validation Accuracy: 0.2653061224489796\n",
      "Fold 8, Epoch 20: Training Loss 1.0351609110832214, Validation Loss 0.9494695663452148, Validation Accuracy: 0.7142857142857143\n",
      "Fold 8, Epoch 30: Training Loss 0.45219341814517977, Validation Loss 0.5877204537391663, Validation Accuracy: 0.8571428571428571\n",
      "Fold 8, Epoch 40: Training Loss 0.17896475195884703, Validation Loss 0.512044370174408, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 50: Training Loss 0.07066377922892571, Validation Loss 0.5229697823524475, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 60: Training Loss 0.03298384267836809, Validation Loss 0.5420615077018738, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 70: Training Loss 0.02148301023989916, Validation Loss 0.5476561188697815, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 80: Training Loss 0.015095506701618433, Validation Loss 0.5683735609054565, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 90: Training Loss 0.011880901735275983, Validation Loss 0.576026976108551, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 100: Training Loss 0.009006714168936013, Validation Loss 0.5959632396697998, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 110: Training Loss 0.0077377131674438715, Validation Loss 0.6045185327529907, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 120: Training Loss 0.006331838155165315, Validation Loss 0.6086073517799377, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 130: Training Loss 0.006457919208332896, Validation Loss 0.6198165416717529, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 140: Training Loss 0.0052523931954056025, Validation Loss 0.6314622759819031, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 150: Training Loss 0.005216509662568569, Validation Loss 0.6410172581672668, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 160: Training Loss 0.004902540892362595, Validation Loss 0.6496627330780029, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 170: Training Loss 0.004536541388370096, Validation Loss 0.6560349464416504, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 180: Training Loss 0.0036133057437837124, Validation Loss 0.6663409471511841, Validation Accuracy: 0.8979591836734694\n",
      "Fold 8, Epoch 190: Training Loss 0.0036937215365469454, Validation Loss 0.6732171773910522, Validation Accuracy: 0.8979591836734694\n",
      "Fold 9, Epoch 0: Training Loss 1.9464298486709595, Validation Loss 1.9159597158432007, Validation Accuracy: 0.3673469387755102\n",
      "Fold 9, Epoch 10: Training Loss 1.6752089858055115, Validation Loss 1.3789920806884766, Validation Accuracy: 0.5714285714285714\n",
      "Fold 9, Epoch 20: Training Loss 0.9514597713947296, Validation Loss 0.7141228318214417, Validation Accuracy: 0.8775510204081632\n",
      "Fold 9, Epoch 30: Training Loss 0.3764079034328461, Validation Loss 0.389446496963501, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 40: Training Loss 0.14206355661153794, Validation Loss 0.3114415109157562, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 50: Training Loss 0.0582604993134737, Validation Loss 0.3032342195510864, Validation Accuracy: 0.9387755102040817\n",
      "Fold 9, Epoch 60: Training Loss 0.031235504895448685, Validation Loss 0.3122626841068268, Validation Accuracy: 0.8979591836734694\n",
      "Fold 9, Epoch 70: Training Loss 0.017475619073957206, Validation Loss 0.3200911581516266, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 80: Training Loss 0.013729410991072655, Validation Loss 0.3233257234096527, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 90: Training Loss 0.010409217234700919, Validation Loss 0.3212551474571228, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 100: Training Loss 0.010368163511157037, Validation Loss 0.3338673710823059, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 110: Training Loss 0.008598311897367239, Validation Loss 0.33823010325431824, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 120: Training Loss 0.007007616711780429, Validation Loss 0.33667677640914917, Validation Accuracy: 0.8979591836734694\n",
      "Fold 9, Epoch 130: Training Loss 0.006460911547765136, Validation Loss 0.3408505916595459, Validation Accuracy: 0.8979591836734694\n",
      "Fold 9, Epoch 140: Training Loss 0.005601701559498906, Validation Loss 0.34853288531303406, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 150: Training Loss 0.004637942276895046, Validation Loss 0.34102603793144226, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 160: Training Loss 0.004270782158710062, Validation Loss 0.3406486213207245, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 170: Training Loss 0.003984129661694169, Validation Loss 0.34896495938301086, Validation Accuracy: 0.9183673469387755\n",
      "Fold 9, Epoch 180: Training Loss 0.003382278932258487, Validation Loss 0.34682807326316833, Validation Accuracy: 0.8979591836734694\n",
      "Fold 9, Epoch 190: Training Loss 0.0032120550284162165, Validation Loss 0.3394811153411865, Validation Accuracy: 0.8979591836734694\n",
      "Fold 10, Epoch 0: Training Loss 1.9739481210708618, Validation Loss 1.9429453611373901, Validation Accuracy: 0.16326530612244897\n",
      "Fold 10, Epoch 10: Training Loss 1.725537085533142, Validation Loss 1.5930906534194946, Validation Accuracy: 0.3469387755102041\n",
      "Fold 10, Epoch 20: Training Loss 1.0515323519706725, Validation Loss 0.9831308722496033, Validation Accuracy: 0.7551020408163265\n",
      "Fold 10, Epoch 30: Training Loss 0.43292289078235624, Validation Loss 0.6786913275718689, Validation Accuracy: 0.7959183673469388\n",
      "Fold 10, Epoch 40: Training Loss 0.16778094097971916, Validation Loss 0.6400677561759949, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 50: Training Loss 0.06905055120587349, Validation Loss 0.6941964030265808, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 60: Training Loss 0.031073947064578535, Validation Loss 0.7304759621620178, Validation Accuracy: 0.8163265306122449\n",
      "Fold 10, Epoch 70: Training Loss 0.020983106922358274, Validation Loss 0.7303366661071777, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 80: Training Loss 0.014592254161834716, Validation Loss 0.7790423035621643, Validation Accuracy: 0.8163265306122449\n",
      "Fold 10, Epoch 90: Training Loss 0.010453049372881652, Validation Loss 0.7685461044311523, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 100: Training Loss 0.008231311663985252, Validation Loss 0.764060914516449, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 110: Training Loss 0.007143650716170668, Validation Loss 0.8101741075515747, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 120: Training Loss 0.007247380167245865, Validation Loss 0.8182721138000488, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 130: Training Loss 0.005344182485714555, Validation Loss 0.8155352473258972, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 140: Training Loss 0.004534969921223819, Validation Loss 0.8408211469650269, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 150: Training Loss 0.004428515955805778, Validation Loss 0.8483019471168518, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 160: Training Loss 0.004120970610529184, Validation Loss 0.8517636656761169, Validation Accuracy: 0.8367346938775511\n",
      "Fold 10, Epoch 170: Training Loss 0.0036107454216107726, Validation Loss 0.8848843574523926, Validation Accuracy: 0.8163265306122449\n",
      "Fold 10, Epoch 180: Training Loss 0.003308031940832734, Validation Loss 0.8882701396942139, Validation Accuracy: 0.8163265306122449\n",
      "Fold 10, Epoch 190: Training Loss 0.002948586386628449, Validation Loss 0.8834869265556335, Validation Accuracy: 0.8367346938775511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full set of labels with a default value (-1)\n",
    "full_labels = torch.full((2480,), -1, dtype=torch.long)\n",
    "full_labels[idx_train] = data.y\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "fold = 0\n",
    "\n",
    "avg_training_losses = []\n",
    "avg_validation_accuracies = []\n",
    "\n",
    "best_validation_accuracy = 0\n",
    "best_validation_loss = float('inf')  # Initialize best validation loss\n",
    "best_model_state = None\n",
    "\n",
    "for train_index, val_index in kf.split(idx_train.numpy()): # folding\n",
    "    fold += 1\n",
    "    model = GraphSAGE(num_features=1390, num_classes=7, hidden_units=64)\n",
    "    optimizer = Adam(model.parameters(), lr=0.011)\n",
    "    training_losses = []\n",
    "    validation_accuracies = []\n",
    "    validation_losses = []  # Track validation loss for each epoch\n",
    "\n",
    "    \n",
    "    for epoch in range(200):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        # compute loss on the training part\n",
    "        train_mask = full_labels[idx_train[train_index]] != -1\n",
    "        loss = F.nll_loss(out[idx_train[train_index]][train_mask], full_labels[idx_train[train_index]][train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # validation accuracy\n",
    "            out = model(data)\n",
    "            _, pred = out.max(1)\n",
    "            val_mask = full_labels[idx_train[val_index]] != -1  # Ensure valid labels for val subset\n",
    "            val_loss = F.nll_loss(out[idx_train[val_index]][val_mask], full_labels[idx_train[val_index]][val_mask]).item()\n",
    "            validation_losses.append(val_loss)  # Store validation loss for this epoch\n",
    "            \n",
    "            # accuracy\n",
    "            correct = pred[idx_train[val_index]][val_mask].eq(full_labels[idx_train[val_index]][val_mask]).sum().item()\n",
    "            accuracy = correct / val_mask.sum().item()\n",
    "            validation_accuracies.append(accuracy)\n",
    "        \n",
    "            # Update best model based on validation loss\n",
    "            if val_loss < best_validation_loss:\n",
    "                best_validation_loss = val_loss\n",
    "                best_validation_accuracy = accuracy\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Fold {fold}, Epoch {epoch}: Training Loss {np.mean(training_losses[-10:])}, Validation Loss {val_loss}, Validation Accuracy: {accuracy}')\n",
    "    \n",
    "    avg_training_losses.append(np.mean(training_losses))\n",
    "    avg_validation_accuracies.append(np.mean(validation_accuracies))\n",
    "\n",
    "# load the best model state\n",
    "model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387755102040817"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3019876182079315"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('conv1.lin_l.weight',\n",
       "              tensor([[ 0.0064, -0.0263,  0.0123,  ..., -0.0211, -0.0128, -0.0816],\n",
       "                      [ 0.0844,  0.0597, -0.0415,  ..., -0.2679, -0.0391,  0.0435],\n",
       "                      [ 0.2282,  0.2783, -0.1716,  ..., -0.0869,  0.3411, -0.1982],\n",
       "                      ...,\n",
       "                      [ 0.0752,  0.4038, -0.0530,  ..., -0.3953,  0.0818,  0.3353],\n",
       "                      [-0.2868,  0.0044,  0.1315,  ..., -0.1185, -0.3480,  0.1689],\n",
       "                      [ 0.0216, -0.0062, -0.0155,  ..., -0.0098,  0.0113, -0.0263]])),\n",
       "             ('conv1.lin_l.bias',\n",
       "              tensor([-0.0812,  0.1357,  0.1614,  0.1796, -0.0665,  0.1211,  0.1273,  0.1290,\n",
       "                       0.1111, -0.0246, -0.0837,  0.1510,  0.1391,  0.1243,  0.1707, -0.0186,\n",
       "                       0.1846,  0.1440,  0.1168, -0.0222,  0.1410,  0.1362,  0.1544,  0.1720,\n",
       "                       0.1219,  0.1454,  0.1446,  0.2195,  0.1093,  0.1213, -0.0221,  0.1397,\n",
       "                       0.1376,  0.1167,  0.1657,  0.1385,  0.1494,  0.1512,  0.1752,  0.1485,\n",
       "                       0.1330,  0.1011,  0.1545,  0.1302,  0.1684,  0.1836,  0.1544,  0.1984,\n",
       "                      -0.0244,  0.1535,  0.1828,  0.1299,  0.1035,  0.1531,  0.1495,  0.1593,\n",
       "                       0.0954, -0.0622,  0.2133,  0.1107,  0.1284,  0.1256,  0.1695, -0.0220])),\n",
       "             ('conv1.lin_r.weight',\n",
       "              tensor([[ 0.0262, -0.0009, -0.0207,  ..., -0.0063, -0.0063, -0.0259],\n",
       "                      [-0.0161,  0.3438, -0.0297,  ..., -0.3195, -0.2152, -0.0009],\n",
       "                      [ 0.1663, -0.0467,  0.3063,  ...,  0.1839,  0.1340,  0.2012],\n",
       "                      ...,\n",
       "                      [ 0.3001,  0.1413,  0.2930,  ..., -0.3278, -0.0024,  0.3031],\n",
       "                      [-0.2431,  0.3315, -0.1904,  ..., -0.3528, -0.0967,  0.0889],\n",
       "                      [-0.0112,  0.0015,  0.0065,  ...,  0.0219,  0.0251, -0.0256]])),\n",
       "             ('conv2.lin_l.weight',\n",
       "              tensor([[-7.7867e-02,  1.0769e-01, -4.1902e-01,  4.4284e-01, -6.5306e-02,\n",
       "                        3.5689e-01,  1.6356e-01,  3.4191e-01, -3.8111e-01,  1.1656e-01,\n",
       "                        1.8286e-01,  4.2074e-01, -3.5831e-02, -3.1815e-01, -4.4764e-01,\n",
       "                        1.1079e-01,  5.1764e-01,  4.7935e-01, -2.0140e-01,  9.8045e-02,\n",
       "                        4.6316e-01,  3.5311e-01,  2.8092e-01, -1.1733e-02,  4.0589e-01,\n",
       "                       -5.4170e-01, -1.7965e-01, -4.2732e-01, -5.0348e-01, -4.3425e-01,\n",
       "                        6.9052e-02, -4.9373e-01, -3.5361e-02, -4.6930e-01, -4.5980e-01,\n",
       "                        1.4925e-01, -3.7403e-01,  3.3073e-01,  4.9311e-01, -3.0377e-01,\n",
       "                       -4.3404e-01,  7.7526e-02, -3.4468e-01,  2.4947e-01,  4.9884e-01,\n",
       "                        5.0388e-01,  2.2636e-01, -3.3643e-01,  3.6950e-02, -3.5735e-01,\n",
       "                        3.9941e-01, -4.4290e-01, -4.9514e-01,  3.9296e-01, -2.5062e-01,\n",
       "                        3.6892e-01, -4.8032e-01,  6.8229e-02, -5.2246e-01,  8.7487e-02,\n",
       "                       -1.5321e-01, -3.4391e-02, -5.5254e-02, -4.8661e-04],\n",
       "                      [ 2.5443e-02,  4.3190e-01,  3.6061e-01,  8.3825e-02,  3.5851e-02,\n",
       "                       -4.1619e-01,  4.1076e-01,  2.9417e-01, -1.4413e-01,  2.0571e-02,\n",
       "                       -5.8551e-04,  4.6226e-01, -2.6245e-01,  4.6708e-01,  4.6078e-01,\n",
       "                        8.7860e-02, -4.4234e-01, -4.6707e-01,  4.8842e-01, -1.0660e-01,\n",
       "                        3.4420e-01, -3.4457e-01, -3.8715e-01, -4.6625e-01, -4.8422e-01,\n",
       "                       -4.6341e-01, -3.3462e-01,  4.6855e-01, -4.2593e-01,  3.0318e-01,\n",
       "                       -1.1007e-01, -2.8854e-01, -3.1471e-01,  3.8775e-01,  4.5176e-01,\n",
       "                        4.0738e-01, -4.5171e-01, -3.6789e-01,  2.9872e-01, -3.1597e-01,\n",
       "                        4.3203e-01, -4.1200e-01,  3.8867e-01, -4.4684e-01,  3.4112e-01,\n",
       "                        4.5754e-01, -4.8179e-01,  4.5621e-01,  3.4769e-02,  4.5210e-01,\n",
       "                       -3.6283e-01, -3.1254e-01, -4.5383e-01, -4.2515e-01, -3.0090e-01,\n",
       "                        3.0335e-01, -4.5597e-01, -8.5802e-02, -3.4410e-01, -4.2664e-01,\n",
       "                       -4.5885e-01, -3.6304e-01, -3.0760e-01, -1.0322e-01],\n",
       "                      [-8.0207e-02,  3.7299e-01,  4.4196e-02,  3.0043e-01,  1.7491e-01,\n",
       "                        3.1073e-01, -3.6626e-01,  3.8542e-02,  3.3814e-01, -9.2694e-03,\n",
       "                       -1.7508e-01, -3.9154e-01,  4.1216e-01,  4.0921e-01, -4.7846e-01,\n",
       "                        1.0955e-01,  3.8176e-02,  1.8000e-01,  3.5072e-01,  3.4392e-02,\n",
       "                       -3.3470e-01, -4.1226e-01, -3.6736e-01,  4.3057e-01, -4.0791e-01,\n",
       "                        5.1967e-02,  2.4135e-01,  1.7017e-01,  4.6326e-01, -2.7251e-01,\n",
       "                        9.7303e-02,  3.4403e-01, -3.1208e-01, -3.6468e-01, -1.3848e-01,\n",
       "                       -8.3564e-02,  1.8906e-01, -1.9276e-01, -2.4058e-01,  3.7570e-01,\n",
       "                        2.5184e-02,  3.1704e-01,  3.1912e-01, -1.1619e-01, -4.3046e-01,\n",
       "                       -3.1691e-01,  6.4484e-02, -3.2934e-01, -1.0015e-01,  4.4207e-01,\n",
       "                       -3.1479e-01,  2.9284e-01,  5.0457e-01,  4.6731e-01,  2.5540e-01,\n",
       "                       -5.7730e-02,  4.1917e-01, -9.0379e-02, -2.7399e-01,  4.2047e-01,\n",
       "                       -2.9661e-01,  5.1117e-01, -1.5329e-01,  3.0728e-02],\n",
       "                      [ 4.0771e-02, -3.5066e-01, -3.7860e-01, -4.9424e-01,  1.7519e-01,\n",
       "                        2.7348e-01,  3.2981e-01, -3.5157e-01,  4.2167e-01,  5.9902e-02,\n",
       "                        1.1344e-02,  7.9152e-02, -4.1794e-01, -3.7416e-01, -4.5762e-01,\n",
       "                       -6.5551e-02,  3.9781e-01, -4.5498e-01, -4.8085e-01, -3.4509e-02,\n",
       "                       -4.5805e-01,  3.5903e-01,  2.8774e-01, -4.7073e-01,  4.3441e-01,\n",
       "                       -4.6667e-01,  4.5090e-01, -4.4933e-01,  4.3440e-01, -3.8988e-01,\n",
       "                        1.1541e-01,  2.6967e-01,  4.2090e-01,  5.1364e-01, -2.6117e-03,\n",
       "                       -4.2445e-01,  4.1382e-01, -3.8250e-01, -1.6380e-01,  4.6785e-01,\n",
       "                        5.0117e-01, -4.8377e-01,  9.3158e-02, -4.8774e-01, -3.5431e-01,\n",
       "                       -3.9759e-01,  4.1025e-01,  3.9156e-01,  2.0152e-03, -4.8505e-01,\n",
       "                        4.6809e-01,  2.3996e-01, -4.9524e-01,  7.5230e-03,  4.7490e-01,\n",
       "                       -3.9154e-01, -3.7473e-01,  3.5537e-01, -2.8276e-01, -3.2109e-01,\n",
       "                        4.4855e-01, -4.8592e-01, -4.8582e-01,  1.0491e-01],\n",
       "                      [ 1.4955e-02, -3.0456e-01,  3.6586e-01, -7.7930e-02,  1.4152e-01,\n",
       "                        2.2390e-01, -5.4005e-01,  1.5090e-02, -4.2552e-01,  4.2238e-03,\n",
       "                       -4.2404e-02, -7.7134e-03, -5.1514e-01,  1.3546e-01,  4.6399e-01,\n",
       "                        9.9305e-02, -5.8315e-01, -5.5750e-01, -2.3211e-01, -1.1641e-01,\n",
       "                        2.9157e-01,  4.7518e-01,  3.0695e-01,  2.9324e-01,  2.5559e-01,\n",
       "                        3.1925e-01, -5.7924e-01,  3.1048e-02, -1.7240e-01, -3.5390e-01,\n",
       "                        2.6354e-02,  2.9259e-01,  3.0517e-01, -1.6493e-01, -5.7724e-01,\n",
       "                        3.0145e-01,  1.1661e-01,  4.0121e-01, -6.0458e-01, -5.2029e-01,\n",
       "                       -1.0931e-01,  2.6505e-01,  2.8583e-01,  3.9206e-01, -4.9882e-01,\n",
       "                       -4.7816e-01,  4.1257e-01,  3.4318e-01, -8.7237e-02, -2.9405e-01,\n",
       "                        2.5956e-01,  1.1058e-01,  2.0094e-01, -3.3764e-01, -1.8737e-01,\n",
       "                        2.8254e-01,  4.0471e-01, -1.3264e-01,  3.6228e-01,  3.1958e-01,\n",
       "                        4.5416e-01, -2.6662e-01, -5.7201e-01,  5.2224e-02],\n",
       "                      [-1.4261e-01, -4.0515e-01, -3.6184e-01,  2.0651e-01,  1.8277e-03,\n",
       "                       -4.5660e-01, -4.8869e-02, -4.8991e-01, -3.7120e-01,  8.3454e-02,\n",
       "                        2.0467e-02,  1.6858e-01,  1.7133e-03, -4.4312e-01, -6.6579e-02,\n",
       "                        2.0842e-02, -4.3634e-01,  3.0566e-01, -4.0533e-01, -1.0927e-01,\n",
       "                       -4.0458e-01, -3.4531e-01,  4.8422e-01, -3.8687e-01,  3.8338e-01,\n",
       "                        3.7596e-01,  1.4753e-01, -2.0335e-01, -3.3512e-02,  3.5821e-01,\n",
       "                        4.2451e-02, -4.0020e-01,  4.5323e-01,  3.3569e-01, -1.5182e-01,\n",
       "                        1.3580e-01, -1.1304e-01,  2.9877e-01,  3.9620e-01,  3.1010e-01,\n",
       "                       -3.0863e-01,  2.2232e-01, -5.1021e-01,  3.6388e-01,  1.6275e-01,\n",
       "                       -6.8886e-02, -4.7937e-01, -4.3230e-01,  3.1638e-02,  2.0906e-01,\n",
       "                       -4.0059e-01, -4.2322e-01, -2.9702e-03, -1.3960e-01, -3.2250e-01,\n",
       "                       -4.5445e-01,  2.8509e-01,  4.3412e-03,  4.8249e-01,  2.7038e-01,\n",
       "                       -4.4656e-01,  3.0104e-01,  4.5844e-01, -7.1630e-02],\n",
       "                      [-1.3266e-01,  1.8341e-01, -4.7605e-01, -5.3689e-01, -1.3584e-01,\n",
       "                       -4.2831e-01,  3.6485e-01,  3.5968e-01,  2.9257e-01,  1.1621e-01,\n",
       "                       -1.6238e-01, -2.9934e-01,  3.8993e-01, -4.1326e-01,  3.2535e-01,\n",
       "                       -1.1694e-01,  3.8702e-01,  2.9752e-01, -4.0898e-01, -9.7872e-02,\n",
       "                       -3.4906e-01, -5.4250e-01, -5.0397e-01,  3.7003e-01, -4.9702e-01,\n",
       "                        4.0370e-01,  1.3108e-02, -5.6574e-01, -4.9933e-01,  4.8687e-01,\n",
       "                        1.0280e-01,  2.9910e-01, -4.6542e-01, -4.2778e-01,  4.9016e-01,\n",
       "                       -4.5386e-01,  4.5786e-01,  4.0344e-01, -3.6241e-01,  1.0391e-01,\n",
       "                       -5.2811e-01, -3.4940e-02, -4.7596e-01,  1.0025e-01,  3.2292e-01,\n",
       "                       -3.7298e-01, -4.0304e-01,  3.7160e-01,  1.9842e-02, -3.6468e-01,\n",
       "                       -4.8130e-01,  4.2668e-01, -4.3904e-01, -4.7748e-01,  4.4785e-01,\n",
       "                        4.0462e-01, -4.6902e-01, -1.2363e-01,  2.3293e-01, -4.2162e-01,\n",
       "                        4.8628e-01, -4.9280e-01,  3.5398e-01, -6.3513e-02]])),\n",
       "             ('conv2.lin_l.bias',\n",
       "              tensor([-0.0728,  0.0790,  0.0710, -0.1129, -0.0972,  0.0290,  0.1538])),\n",
       "             ('conv2.lin_r.weight',\n",
       "              tensor([[-0.0055,  0.1433, -0.4395,  0.4614, -0.0794,  0.4808, -0.4578,  0.4580,\n",
       "                       -0.3675,  0.1211, -0.0207,  0.4283, -0.5688, -0.2675, -0.4615,  0.1172,\n",
       "                        0.4879,  0.4850,  0.0968, -0.0433,  0.4942,  0.4470,  0.4065, -0.3429,\n",
       "                        0.3742, -0.4827, -0.4093, -0.3556, -0.4733, -0.2761, -0.0765, -0.3782,\n",
       "                        0.4714, -0.3213, -0.4641,  0.5214, -0.4590,  0.3779,  0.5055, -0.5118,\n",
       "                       -0.3670,  0.4562, -0.5314,  0.4251,  0.3518,  0.4689,  0.2590, -0.5539,\n",
       "                        0.0683, -0.5192,  0.3484, -0.4593,  0.3449,  0.4968, -0.4386,  0.2238,\n",
       "                       -0.5419, -0.1627, -0.4061,  0.3377, -0.4041, -0.4305,  0.4554, -0.0877],\n",
       "                      [-0.0591,  0.4867,  0.3758, -0.3956,  0.0277, -0.3989,  0.1516, -0.2479,\n",
       "                       -0.2712,  0.0578, -0.0169,  0.3791, -0.4979,  0.3676,  0.4534,  0.0376,\n",
       "                       -0.4307, -0.3422,  0.3414,  0.0016,  0.4139, -0.5010, -0.4081, -0.4124,\n",
       "                       -0.4288, -0.1540, -0.3130,  0.4783, -0.2910, -0.3928,  0.0363,  0.0872,\n",
       "                       -0.4914,  0.3961,  0.3861,  0.4143, -0.4421, -0.3384,  0.5054, -0.3932,\n",
       "                        0.4880, -0.3926,  0.4224, -0.4789,  0.4741,  0.3570, -0.4386,  0.2617,\n",
       "                       -0.1191,  0.4290, -0.4550, -0.2765, -0.2320, -0.2889, -0.4869,  0.4608,\n",
       "                       -0.3902,  0.0183,  0.1142, -0.3078, -0.4667, -0.3305,  0.2538, -0.0612],\n",
       "                      [-0.0455,  0.3804,  0.4510,  0.0779, -0.0444,  0.4662, -0.3830, -0.3715,\n",
       "                        0.4521, -0.0689,  0.0501, -0.4496,  0.3576,  0.3296, -0.4723, -0.0720,\n",
       "                        0.2934, -0.0170,  0.4048,  0.0104, -0.2657, -0.3911, -0.4537,  0.0014,\n",
       "                       -0.3541, -0.4324,  0.4951, -0.1256,  0.2495,  0.2192,  0.1106,  0.3196,\n",
       "                       -0.4235, -0.5052,  0.2518, -0.4703,  0.3905, -0.3332, -0.5003,  0.4182,\n",
       "                        0.3337,  0.4447,  0.2398,  0.4272, -0.4220, -0.3460, -0.4141, -0.1867,\n",
       "                        0.0479,  0.3932,  0.1788,  0.4850,  0.4515,  0.4301,  0.1109,  0.2871,\n",
       "                        0.2590,  0.0380, -0.4041,  0.5058,  0.0149,  0.4045, -0.3177, -0.1152],\n",
       "                      [ 0.0086, -0.4636,  0.2984, -0.3776, -0.1127,  0.4148,  0.5084, -0.4531,\n",
       "                        0.2327, -0.0729,  0.0411,  0.5338,  0.2287, -0.2808, -0.3897, -0.0818,\n",
       "                        0.4165,  0.3411, -0.1995, -0.0529, -0.3143,  0.4513,  0.5105, -0.5084,\n",
       "                        0.3742, -0.3987,  0.4186, -0.1681,  0.3570, -0.4583, -0.0572,  0.3422,\n",
       "                        0.4978,  0.3745,  0.4565, -0.3911,  0.2843, -0.3945,  0.4971,  0.2767,\n",
       "                        0.3882, -0.3887,  0.4978, -0.4976, -0.3501, -0.4545,  0.3956,  0.2655,\n",
       "                        0.0127,  0.2077,  0.3619,  0.4030, -0.3123, -0.1457,  0.5121, -0.4792,\n",
       "                       -0.5004,  0.0752, -0.3721, -0.5638,  0.3194, -0.4244, -0.2683,  0.0389],\n",
       "                      [-0.1719, -0.3323,  0.2403,  0.4602, -0.0257, -0.4944, -0.3608,  0.4708,\n",
       "                       -0.1862, -0.0641,  0.0904,  0.3040, -0.5512,  0.3917,  0.2615,  0.0789,\n",
       "                       -0.5561, -0.4767, -0.2829,  0.0662, -0.3650,  0.1796, -0.1167,  0.3417,\n",
       "                        0.3795,  0.3325, -0.4313,  0.3970, -0.1217,  0.2620, -0.0699,  0.0081,\n",
       "                        0.3316, -0.3384, -0.5622, -0.4573,  0.1129,  0.4600, -0.4849, -0.6086,\n",
       "                       -0.4999,  0.4667,  0.3058, -0.2642, -0.5787, -0.4813,  0.3436,  0.3872,\n",
       "                        0.0853, -0.4667,  0.4877, -0.5262,  0.0166, -0.4429, -0.4236, -0.3744,\n",
       "                       -0.2241, -0.1110,  0.4618, -0.4918,  0.3767, -0.2166, -0.4942, -0.0559],\n",
       "                      [-0.0069, -0.3888, -0.4919,  0.2592,  0.0441, -0.4718,  0.3800, -0.5045,\n",
       "                       -0.2114, -0.1107, -0.0422, -0.4188,  0.2079, -0.3580, -0.2566, -0.0387,\n",
       "                       -0.3819, -0.4215, -0.2364, -0.0104, -0.5519, -0.5496,  0.1445, -0.1419,\n",
       "                        0.3678,  0.4736, -0.2632, -0.4879,  0.3618,  0.4722,  0.0948, -0.4361,\n",
       "                        0.3837,  0.2809, -0.4663, -0.1456, -0.4707,  0.2347, -0.1416,  0.3474,\n",
       "                       -0.4391, -0.5162, -0.3347,  0.4757, -0.4217,  0.2908, -0.4925, -0.5423,\n",
       "                       -0.1227,  0.3233, -0.5036, -0.3292,  0.1593, -0.2229, -0.5025, -0.4779,\n",
       "                        0.4632,  0.0488,  0.4936, -0.5930, -0.5538,  0.1963,  0.3269, -0.0234],\n",
       "                      [-0.0283, -0.0382, -0.4253, -0.3543, -0.0842, -0.5037,  0.4908,  0.4048,\n",
       "                        0.0021,  0.0576, -0.0303, -0.4939,  0.2182, -0.4210,  0.4277,  0.0136,\n",
       "                        0.4432, -0.4767, -0.3543,  0.0646, -0.4174, -0.3106, -0.3585,  0.5010,\n",
       "                       -0.5350,  0.2556,  0.1958, -0.2437, -0.5044,  0.4269,  0.1076, -0.0012,\n",
       "                       -0.5802, -0.4790,  0.2301,  0.4814,  0.3307,  0.4032, -0.4205, -0.5018,\n",
       "                       -0.4203, -0.5392, -0.5367, -0.3379,  0.4492, -0.5678,  0.4006,  0.3202,\n",
       "                       -0.1000, -0.5288, -0.5584,  0.4498, -0.4932, -0.1637,  0.4607,  0.3371,\n",
       "                        0.3618,  0.1270,  0.2117, -0.5821,  0.3827, -0.3941,  0.4624, -0.0056]]))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(data)\n",
    "    _, pred = out.max(1)\n",
    "    test_labels_pred = pred[idx_test].numpy()\n",
    "submission_file_path = 'submission_graphSage.txt'\n",
    "with open(submission_file_path, 'w') as file:\n",
    "    for label in test_labels_pred:\n",
    "        file.write(f'{label}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
